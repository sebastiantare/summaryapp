[2023-07-11T22:49:43.359+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data manual__2023-07-11T22:49:40.905784+00:00 [queued]>
[2023-07-11T22:49:43.365+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data manual__2023-07-11T22:49:40.905784+00:00 [queued]>
[2023-07-11T22:49:43.365+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T22:49:43.376+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonVirtualenvDecoratedOperator): scrap_data> on 2023-07-11 22:49:40.905784+00:00
[2023-07-11T22:49:43.380+0000] {standard_task_runner.py:57} INFO - Started process 353 to run task
[2023-07-11T22:49:43.383+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'manual__2023-07-11T22:49:40.905784+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmp08zsl51x']
[2023-07-11T22:49:43.383+0000] {standard_task_runner.py:85} INFO - Job 6: Subtask scrap_data
[2023-07-11T22:49:43.414+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data manual__2023-07-11T22:49:40.905784+00:00 [running]> on host 5a2aa5f79b19
[2023-07-11T22:49:43.478+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-11T22:49:40.905784+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-07-11T22:49:40.905784+00:00'
[2023-07-11T22:49:43.479+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python -m virtualenv /tmp/venv_19luw1z --system-site-packages --python=python3.9
[2023-07-11T22:49:43.490+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:49:45.054+0000] {process_utils.py:189} INFO - created virtual environment CPython3.9.2.final.0-64 in 984ms
[2023-07-11T22:49:45.055+0000] {process_utils.py:189} INFO -   creator CPython3Posix(dest=/tmp/venv_19luw1z, clear=False, no_vcs_ignore=False, global=True)
[2023-07-11T22:49:45.055+0000] {process_utils.py:189} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/tmp/tmpu5evizxu)
[2023-07-11T22:49:45.055+0000] {process_utils.py:189} INFO -     added seed packages: pip==23.1, setuptools==67.6.1, wheel==0.40.0
[2023-07-11T22:49:45.055+0000] {process_utils.py:189} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2023-07-11T22:49:45.083+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venv_19luw1z/bin/pip install -r /tmp/venv_19luw1z/requirements.txt
[2023-07-11T22:49:45.090+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:49:45.616+0000] {process_utils.py:189} INFO - WARNING: The directory '/home/***/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.
[2023-07-11T22:49:45.819+0000] {process_utils.py:189} INFO - Collecting selenium (from -r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:45.962+0000] {process_utils.py:189} INFO -   Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)
[2023-07-11T22:49:46.169+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 32.9 MB/s eta 0:00:00
[2023-07-11T22:49:46.209+0000] {process_utils.py:189} INFO - Collecting pyspark (from -r /tmp/venv_19luw1z/requirements.txt (line 2))
[2023-07-11T22:49:46.239+0000] {process_utils.py:189} INFO -   Downloading pyspark-3.4.1.tar.gz (310.8 MB)
[2023-07-11T22:49:53.205+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 310.8/310.8 MB 47.6 MB/s eta 0:00:00
[2023-07-11T22:49:56.276+0000] {process_utils.py:189} INFO -   Preparing metadata (setup.py): started
[2023-07-11T22:49:56.499+0000] {process_utils.py:189} INFO -   Preparing metadata (setup.py): finished with status 'done'
[2023-07-11T22:49:56.565+0000] {process_utils.py:189} INFO - Collecting dateparser (from -r /tmp/venv_19luw1z/requirements.txt (line 3))
[2023-07-11T22:49:56.597+0000] {process_utils.py:189} INFO -   Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)
[2023-07-11T22:49:56.628+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.8/293.8 kB 199.1 MB/s eta 0:00:00
[2023-07-11T22:49:56.714+0000] {process_utils.py:189} INFO - Collecting urllib3[socks]<3,>=1.26 (from selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:56.745+0000] {process_utils.py:189} INFO -   Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)
[2023-07-11T22:49:56.751+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.6/123.6 kB 23.4 MB/s eta 0:00:00
[2023-07-11T22:49:56.801+0000] {process_utils.py:189} INFO - Collecting trio~=0.17 (from selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:56.839+0000] {process_utils.py:189} INFO -   Downloading trio-0.22.1-py3-none-any.whl (399 kB)
[2023-07-11T22:49:56.849+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 399.3/399.3 kB 45.8 MB/s eta 0:00:00
[2023-07-11T22:49:56.890+0000] {process_utils.py:189} INFO - Collecting trio-websocket~=0.9 (from selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:56.916+0000] {process_utils.py:189} INFO -   Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)
[2023-07-11T22:49:56.960+0000] {process_utils.py:189} INFO - Collecting certifi>=2021.10.8 (from selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:56.985+0000] {process_utils.py:189} INFO -   Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)
[2023-07-11T22:49:56.992+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.0/157.0 kB 34.9 MB/s eta 0:00:00
[2023-07-11T22:49:57.049+0000] {process_utils.py:189} INFO - Collecting py4j==0.10.9.7 (from pyspark->-r /tmp/venv_19luw1z/requirements.txt (line 2))
[2023-07-11T22:49:57.075+0000] {process_utils.py:189} INFO -   Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)
[2023-07-11T22:49:57.086+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 kB 22.8 MB/s eta 0:00:00
[2023-07-11T22:49:57.149+0000] {process_utils.py:189} INFO - Collecting python-dateutil (from dateparser->-r /tmp/venv_19luw1z/requirements.txt (line 3))
[2023-07-11T22:49:57.176+0000] {process_utils.py:189} INFO -   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
[2023-07-11T22:49:57.184+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 43.5 MB/s eta 0:00:00
[2023-07-11T22:49:57.268+0000] {process_utils.py:189} INFO - Collecting pytz (from dateparser->-r /tmp/venv_19luw1z/requirements.txt (line 3))
[2023-07-11T22:49:57.296+0000] {process_utils.py:189} INFO -   Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)
[2023-07-11T22:49:57.330+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.3/502.3 kB 249.2 MB/s eta 0:00:00
[2023-07-11T22:49:57.894+0000] {process_utils.py:189} INFO - Collecting regex!=2019.02.19,!=2021.8.27 (from dateparser->-r /tmp/venv_19luw1z/requirements.txt (line 3))
[2023-07-11T22:49:57.924+0000] {process_utils.py:189} INFO -   Downloading regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)
[2023-07-11T22:49:57.939+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.9/769.9 kB 56.1 MB/s eta 0:00:00
[2023-07-11T22:49:57.978+0000] {process_utils.py:189} INFO - Collecting tzlocal (from dateparser->-r /tmp/venv_19luw1z/requirements.txt (line 3))
[2023-07-11T22:49:58.012+0000] {process_utils.py:189} INFO -   Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)
[2023-07-11T22:49:58.068+0000] {process_utils.py:189} INFO - Collecting attrs>=20.1.0 (from trio~=0.17->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.111+0000] {process_utils.py:189} INFO -   Downloading attrs-23.1.0-py3-none-any.whl (61 kB)
[2023-07-11T22:49:58.115+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 35.0 MB/s eta 0:00:00
[2023-07-11T22:49:58.158+0000] {process_utils.py:189} INFO - Collecting sortedcontainers (from trio~=0.17->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.180+0000] {process_utils.py:189} INFO -   Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
[2023-07-11T22:49:58.218+0000] {process_utils.py:189} INFO - Collecting idna (from trio~=0.17->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.242+0000] {process_utils.py:189} INFO -   Downloading idna-3.4-py3-none-any.whl (61 kB)
[2023-07-11T22:49:58.246+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 33.6 MB/s eta 0:00:00
[2023-07-11T22:49:58.316+0000] {process_utils.py:189} INFO - Collecting outcome (from trio~=0.17->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.343+0000] {process_utils.py:189} INFO -   Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)
[2023-07-11T22:49:58.383+0000] {process_utils.py:189} INFO - Collecting sniffio (from trio~=0.17->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.409+0000] {process_utils.py:189} INFO -   Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)
[2023-07-11T22:49:58.456+0000] {process_utils.py:189} INFO - Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.493+0000] {process_utils.py:189} INFO -   Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)
[2023-07-11T22:49:58.542+0000] {process_utils.py:189} INFO - Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.573+0000] {process_utils.py:189} INFO -   Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)
[2023-07-11T22:49:58.752+0000] {process_utils.py:189} INFO - Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.775+0000] {process_utils.py:189} INFO -   Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)
[2023-07-11T22:49:58.825+0000] {process_utils.py:189} INFO - Collecting six>=1.5 (from python-dateutil->dateparser->-r /tmp/venv_19luw1z/requirements.txt (line 3))
[2023-07-11T22:49:58.850+0000] {process_utils.py:189} INFO -   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
[2023-07-11T22:49:58.936+0000] {process_utils.py:189} INFO - Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r /tmp/venv_19luw1z/requirements.txt (line 1))
[2023-07-11T22:49:58.969+0000] {process_utils.py:189} INFO -   Downloading h11-0.14.0-py3-none-any.whl (58 kB)
[2023-07-11T22:49:58.972+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 33.4 MB/s eta 0:00:00
[2023-07-11T22:49:58.994+0000] {process_utils.py:189} INFO - Building wheels for collected packages: pyspark
[2023-07-11T22:49:58.994+0000] {process_utils.py:189} INFO -   Building wheel for pyspark (setup.py): started
[2023-07-11T22:50:16.578+0000] {process_utils.py:189} INFO -   Building wheel for pyspark (setup.py): finished with status 'done'
[2023-07-11T22:50:16.785+0000] {process_utils.py:189} INFO -   Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=7005d88648fbc477bd0382967678496931725df9edb3cf91ecdc34312f3e960f
[2023-07-11T22:50:16.785+0000] {process_utils.py:189} INFO -   Stored in directory: /tmp/pip-ephem-wheel-cache-3hx10jjk/wheels/2b/9a/39/d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8
[2023-07-11T22:50:16.790+0000] {process_utils.py:189} INFO - Successfully built pyspark
[2023-07-11T22:50:16.992+0000] {process_utils.py:189} INFO - Installing collected packages: sortedcontainers, pytz, py4j, urllib3, tzlocal, sniffio, six, regex, pyspark, pysocks, idna, h11, exceptiongroup, certifi, attrs, wsproto, python-dateutil, outcome, trio, dateparser, trio-websocket, selenium
[2023-07-11T22:50:21.499+0000] {process_utils.py:189} INFO - Successfully installed attrs-23.1.0 certifi-2023.5.7 dateparser-1.1.8 exceptiongroup-1.1.2 h11-0.14.0 idna-3.4 outcome-1.2.0 py4j-0.10.9.7 pysocks-1.7.1 pyspark-3.4.1 python-dateutil-2.8.2 pytz-2023.3 regex-2023.6.3 selenium-4.10.0 six-1.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.22.1 trio-websocket-0.10.3 tzlocal-5.0.1 urllib3-2.0.3 wsproto-1.2.0
[2023-07-11T22:50:21.622+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:21.623+0000] {process_utils.py:189} INFO - [notice] A new release of pip is available: 23.1 -> 23.1.2
[2023-07-11T22:50:21.623+0000] {process_utils.py:189} INFO - [notice] To update, run: /tmp/venv_19luw1z/bin/python -m pip install --upgrade pip
[2023-07-11T22:50:21.851+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venv_19luw1z/bin/python /tmp/venv_19luw1z/script.py /tmp/venv_19luw1z/script.in /tmp/venv_19luw1z/script.out /tmp/venv_19luw1z/string_args.txt
[2023-07-11T22:50:21.861+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:50:22.185+0000] {process_utils.py:189} INFO - /tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-11T22:50:23.534+0000] {process_utils.py:189} INFO - Setting default log level to "WARN".
[2023-07-11T22:50:23.535+0000] {process_utils.py:189} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2023-07-11T22:50:23.673+0000] {process_utils.py:189} INFO - 23/07/11 22:50:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO - [Stage 0:>                                                          (0 + 1) / 1]23/07/11 22:50:31 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:31.894+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:50:31.895+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:50:31.896+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:50:31.897+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:50:31.897+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:50:31.897+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:50:31.897+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:50:31.912+0000] {process_utils.py:189} INFO - 23/07/11 22:50:31 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (5a2aa5f79b19 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:50:31.912+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:50:31.912+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:50:31.912+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:50:31.912+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:31.912+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:31.913+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:50:31.914+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:50:31.915+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:50:31.916+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:50:31.916+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:50:31.916+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:50:31.916+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:50:31.916+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:31.916+0000] {process_utils.py:189} INFO - 23/07/11 22:50:31 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2023-07-11T22:50:32.164+0000] {process_utils.py:189} INFO - /tmp/venv_19luw1z/bin/python
[2023-07-11T22:50:32.164+0000] {process_utils.py:189} INFO - ['/tmp/venv_19luw1z', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '/tmp/venv_19luw1z/lib/python3.9/site-packages', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages']
[2023-07-11T22:50:32.164+0000] {process_utils.py:189} INFO - 1) BROWSER OK
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - 2) GET OK
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - 3) BUTTON OK
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - 4) CLICK OK
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - 5) FETCH OK
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - 6) FETCH 2 OK
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - Loading data...
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/script.py", line 162, in <module>
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO -     res = scrapData(*arg_dict["args"], **arg_dict["kwargs"])
[2023-07-11T22:50:32.165+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/script.py", line 157, in scrapData
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -     data_df.show()
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 899, in show
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -     print(self._jdf.showString(n, 20, vertical))
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -     return_value = get_return_value(
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
[2023-07-11T22:50:32.166+0000] {process_utils.py:189} INFO -     return f(*a, **kw)
[2023-07-11T22:50:32.167+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/py4j/protocol.py", line 326, in get_return_value
[2023-07-11T22:50:32.167+0000] {process_utils.py:189} INFO -     raise Py4JJavaError(
[2023-07-11T22:50:32.207+0000] {process_utils.py:189} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o38.showString.
[2023-07-11T22:50:32.207+0000] {process_utils.py:189} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (5a2aa5f79b19 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:50:32.207+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:50:32.207+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:32.208+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:50:32.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - Driver stacktrace:
[2023-07-11T22:50:32.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
[2023-07-11T22:50:32.211+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
[2023-07-11T22:50:32.211+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
[2023-07-11T22:50:32.211+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-07-11T22:50:32.211+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-07-11T22:50:32.211+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-07-11T22:50:32.211+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at scala.Option.foreach(Option.scala:407)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-07-11T22:50:32.212+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
[2023-07-11T22:50:32.213+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
[2023-07-11T22:50:32.214+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-07-11T22:50:32.215+0000] {process_utils.py:189} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO -   File "/tmp/venv_19luw1z/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:50:32.216+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:50:32.217+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:50:32.218+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 	... 1 more
[2023-07-11T22:50:32.219+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:50:32.815+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 578, in execute_callable
    result = self._execute_python_callable_in_subprocess(python_path, tmp_path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 434, in _execute_python_callable_in_subprocess
    os.fspath(string_args_path),
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/tmp/venv_19luw1z/bin/python', '/tmp/venv_19luw1z/script.py', '/tmp/venv_19luw1z/script.in', '/tmp/venv_19luw1z/script.out', '/tmp/venv_19luw1z/string_args.txt']' returned non-zero exit status 1.
[2023-07-11T22:50:32.824+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230711T224940, start_date=20230711T224943, end_date=20230711T225032
[2023-07-11T22:50:32.833+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 6 for task scrap_data (Command '['/tmp/venv_19luw1z/bin/python', '/tmp/venv_19luw1z/script.py', '/tmp/venv_19luw1z/script.in', '/tmp/venv_19luw1z/script.out', '/tmp/venv_19luw1z/string_args.txt']' returned non-zero exit status 1.; 353)
[2023-07-11T22:50:32.863+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T22:50:32.876+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
