[2023-07-11T00:00:02.657+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T00:00:02.663+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T00:00:02.663+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T00:00:02.673+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonExternalDecoratedOperator): scrap_data> on 2023-07-10 00:00:00+00:00
[2023-07-11T00:00:02.678+0000] {standard_task_runner.py:57} INFO - Started process 10864 to run task
[2023-07-11T00:00:02.679+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '191', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmpsbj6nwn7']
[2023-07-11T00:00:02.680+0000] {standard_task_runner.py:85} INFO - Job 191: Subtask scrap_data
[2023-07-11T00:00:02.713+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [running]> on host 3371e7602d27
[2023-07-11T00:00:02.771+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-11T00:00:03.379+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python3.7 /tmp/tmd3xpi1_a1/script.py /tmp/tmd3xpi1_a1/script.in /tmp/tmd3xpi1_a1/script.out /tmp/tmd3xpi1_a1/string_args.txt
[2023-07-11T00:00:03.387+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T00:00:04.355+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T00:00:04.355+0000] {process_utils.py:189} INFO -   File "/tmp/tmd3xpi1_a1/script.py", line 160, in <module>
[2023-07-11T00:00:04.356+0000] {process_utils.py:189} INFO -     res = scrapData(*arg_dict["args"], **arg_dict["kwargs"])
[2023-07-11T00:00:04.356+0000] {process_utils.py:189} INFO -   File "/tmp/tmd3xpi1_a1/script.py", line 37, in scrapData
[2023-07-11T00:00:04.356+0000] {process_utils.py:189} INFO -     from pyspark.sql import SparkSession
[2023-07-11T00:00:04.356+0000] {process_utils.py:189} INFO - ModuleNotFoundError: No module named 'pyspark'
[2023-07-11T00:00:04.410+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 696, in execute_callable
    return self._execute_python_callable_in_subprocess(python_path, tmp_path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 434, in _execute_python_callable_in_subprocess
    os.fspath(string_args_path),
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/usr/local/bin/python3.7', '/tmp/tmd3xpi1_a1/script.py', '/tmp/tmd3xpi1_a1/script.in', '/tmp/tmd3xpi1_a1/script.out', '/tmp/tmd3xpi1_a1/string_args.txt']' returned non-zero exit status 1.
[2023-07-11T00:00:04.417+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230710T000000, start_date=20230711T000002, end_date=20230711T000004
[2023-07-11T00:00:04.426+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 191 for task scrap_data (Command '['/usr/local/bin/python3.7', '/tmp/tmd3xpi1_a1/script.py', '/tmp/tmd3xpi1_a1/script.in', '/tmp/tmd3xpi1_a1/script.out', '/tmp/tmd3xpi1_a1/string_args.txt']' returned non-zero exit status 1.; 10864)
[2023-07-11T00:00:04.460+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T00:00:04.476+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-11T17:43:13.708+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T17:43:13.715+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T17:43:13.715+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T17:43:13.727+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonExternalDecoratedOperator): scrap_data> on 2023-07-10 00:00:00+00:00
[2023-07-11T17:43:13.732+0000] {standard_task_runner.py:57} INFO - Started process 273 to run task
[2023-07-11T17:43:13.734+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmpsy0w10fg']
[2023-07-11T17:43:13.734+0000] {standard_task_runner.py:85} INFO - Job 8: Subtask scrap_data
[2023-07-11T17:43:13.775+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [running]> on host 6d0768751585
[2023-07-11T17:43:13.851+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-11T17:43:14.517+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python3.7 /tmp/tmd4mt18krw/script.py /tmp/tmd4mt18krw/script.in /tmp/tmd4mt18krw/script.out /tmp/tmd4mt18krw/string_args.txt
[2023-07-11T17:43:14.527+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T17:43:16.080+0000] {process_utils.py:189} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-11T17:43:18.192+0000] {process_utils.py:189} INFO - Setting default log level to "WARN".
[2023-07-11T17:43:18.193+0000] {process_utils.py:189} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2023-07-11T17:43:18.389+0000] {process_utils.py:189} INFO - 23/07/11 17:43:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-11T17:43:19.516+0000] {process_utils.py:189} INFO - /home/***/.local/lib/python3.7/site-packages/pyspark/context.py:317 FutureWarning: Python 3.7 support is deprecated in Spark 3.4.
[2023-07-11T17:43:31.193+0000] {process_utils.py:189} INFO - [Stage 0:>                                                          (0 + 1) / 1]                                                                                1) BROWSER OK
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - 2) GET OK
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - 3) BUTTON OK
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - 4) CLICK OK
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - 5) FETCH OK
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - 6) FETCH 2 OK
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - Loading data...
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.194+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.195+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.196+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - +--------------------+--------------------+--------+------------+------------+-----------+--------------+--------------------+-----------------+--------------+----------------+
[2023-07-11T17:43:31.197+0000] {process_utils.py:189} INFO - |        article_hash|       article_title|category|publish_date|article_body|raw_content| source_entity|        article_link|generated_summary|negative_score|importance_score|
[2023-07-11T17:43:31.198+0000] {process_utils.py:189} INFO - +--------------------+--------------------+--------+------------+------------+-----------+--------------+--------------------+-----------------+--------------+----------------+
[2023-07-11T17:43:31.198+0000] {process_utils.py:189} INFO - |0b96189e595cecd44...|Diputada Pérez (R...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.198+0000] {process_utils.py:189} INFO - |b2610f43a2efaf002...|Detienen a mamá d...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.198+0000] {process_utils.py:189} INFO - |fa6c8abd209edc034...|"No venga a vesti...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.199+0000] {process_utils.py:189} INFO - |57be750835115edcc...|Operación Huracán...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.199+0000] {process_utils.py:189} INFO - |cafe2f5e4a38cda1c...|Becker y AC contr...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.199+0000] {process_utils.py:189} INFO - |ac8afd91faf50e02b...|Fiscalía agrava c...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.199+0000] {process_utils.py:189} INFO - |88dd6b55c8093ef97...|"Una pesadilla": ...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.200+0000] {process_utils.py:189} INFO - |3a0d1735e3b05ac0b...|"El Presidente no...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.200+0000] {process_utils.py:189} INFO - |66f2dbffcd02c5d08...|"Hay muchas denun...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.200+0000] {process_utils.py:189} INFO - |1d622161c13a8b3e1...|Lo trató de "perv...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.200+0000] {process_utils.py:189} INFO - |3c8580a2f22fd53ed...|¿Suspenderá la li...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.201+0000] {process_utils.py:189} INFO - |ab1497f5de0ce5fe4...|PDI incorpora nue...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.201+0000] {process_utils.py:189} INFO - |84bd79a9ff0143264...|Banco Central con...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.201+0000] {process_utils.py:189} INFO - |2c888607b0f89f983...|Autoridades busca...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.201+0000] {process_utils.py:189} INFO - |c5d66ddaab8e33c19...|Atropellan a cara...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.202+0000] {process_utils.py:189} INFO - |4dd41b21e6cf91303...|Cordero asegura q...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.202+0000] {process_utils.py:189} INFO - |6d53ce7e9be378f45...|Exministra Isabel...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.202+0000] {process_utils.py:189} INFO - |a333edfbd8213b827...|Privados, mérito ...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.202+0000] {process_utils.py:189} INFO - |5cfde110e7bc5dede...|Comisión Revisora...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.203+0000] {process_utils.py:189} INFO - |d59acd1730d3ff299...|Valdivia: vecinos...|Nacional|        null|        null|       null|biobiochile.cl|https://www.biobi...|             null|          null|            null|
[2023-07-11T17:43:31.203+0000] {process_utils.py:189} INFO - +--------------------+--------------------+--------+------------+------------+-----------+--------------+--------------------+-----------------+--------------+----------------+
[2023-07-11T17:43:31.203+0000] {process_utils.py:189} INFO - only showing top 20 rows
[2023-07-11T17:43:31.203+0000] {process_utils.py:189} INFO - 
[2023-07-11T17:43:31.203+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T17:43:31.203+0000] {process_utils.py:189} INFO -   File "/tmp/tmd4mt18krw/script.py", line 160, in <module>
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -     res = scrapData(*arg_dict["args"], **arg_dict["kwargs"])
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -   File "/tmp/tmd4mt18krw/script.py", line 156, in scrapData
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -     writeToDB(data_df)
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -   File "/tmp/tmd4mt18krw/script.py", line 76, in writeToDB
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -     .mode("append") \
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -   File "/home/***/.local/lib/python3.7/site-packages/pyspark/sql/readwriter.py", line 1396, in save
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -     self._jwrite.save()
[2023-07-11T17:43:31.204+0000] {process_utils.py:189} INFO -   File "/home/***/.local/lib/python3.7/site-packages/py4j/java_gateway.py", line 1323, in __call__
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO -     answer, self.gateway_client, self.target_id, self.name)
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO -   File "/home/***/.local/lib/python3.7/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO -     return f(*a, **kw)
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO -   File "/home/***/.local/lib/python3.7/site-packages/py4j/protocol.py", line 328, in get_return_value
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO -     format(target_id, ".", name), value)
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o45.save.
[2023-07-11T17:43:31.205+0000] {process_utils.py:189} INFO - : java.sql.SQLException: No suitable driver
[2023-07-11T17:43:31.206+0000] {process_utils.py:189} INFO - 	at java.sql/java.sql.DriverManager.getDriver(DriverManager.java:298)
[2023-07-11T17:43:31.206+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$2(JDBCOptions.scala:109)
[2023-07-11T17:43:31.206+0000] {process_utils.py:189} INFO - 	at scala.Option.getOrElse(Option.scala:189)
[2023-07-11T17:43:31.206+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:109)
[2023-07-11T17:43:31.206+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:246)
[2023-07-11T17:43:31.206+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:250)
[2023-07-11T17:43:31.207+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:47)
[2023-07-11T17:43:31.207+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
[2023-07-11T17:43:31.207+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[2023-07-11T17:43:31.207+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[2023-07-11T17:43:31.207+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[2023-07-11T17:43:31.207+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
[2023-07-11T17:43:31.208+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
[2023-07-11T17:43:31.209+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-07-11T17:43:31.210+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T17:43:31.211+0000] {process_utils.py:189} INFO - 
[2023-07-11T17:43:31.877+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 696, in execute_callable
    return self._execute_python_callable_in_subprocess(python_path, tmp_path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 434, in _execute_python_callable_in_subprocess
    os.fspath(string_args_path),
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/usr/local/bin/python3.7', '/tmp/tmd4mt18krw/script.py', '/tmp/tmd4mt18krw/script.in', '/tmp/tmd4mt18krw/script.out', '/tmp/tmd4mt18krw/string_args.txt']' returned non-zero exit status 1.
[2023-07-11T17:43:31.886+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230710T000000, start_date=20230711T174313, end_date=20230711T174331
[2023-07-11T17:43:31.897+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 8 for task scrap_data (Command '['/usr/local/bin/python3.7', '/tmp/tmd4mt18krw/script.py', '/tmp/tmd4mt18krw/script.in', '/tmp/tmd4mt18krw/script.out', '/tmp/tmd4mt18krw/string_args.txt']' returned non-zero exit status 1.; 273)
[2023-07-11T17:43:31.913+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T17:43:31.925+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-11T21:47:55.171+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T21:47:55.184+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T21:47:55.184+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T21:47:55.195+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonVirtualenvDecoratedOperator): scrap_data> on 2023-07-10 00:00:00+00:00
[2023-07-11T21:47:55.200+0000] {standard_task_runner.py:57} INFO - Started process 81 to run task
[2023-07-11T21:47:55.203+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmpeegexlfs']
[2023-07-11T21:47:55.203+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask scrap_data
[2023-07-11T21:47:55.239+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [running]> on host efcec7422c3c
[2023-07-11T21:47:55.306+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-11T21:47:55.307+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python -m virtualenv /tmp/venvmaoe874m --system-site-packages --python=python3.9
[2023-07-11T21:47:55.318+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T21:47:57.133+0000] {process_utils.py:189} INFO - created virtual environment CPython3.9.2.final.0-64 in 1184ms
[2023-07-11T21:47:57.133+0000] {process_utils.py:189} INFO -   creator CPython3Posix(dest=/tmp/venvmaoe874m, clear=False, no_vcs_ignore=False, global=True)
[2023-07-11T21:47:57.133+0000] {process_utils.py:189} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/home/***/.local/share/virtualenv)
[2023-07-11T21:47:57.134+0000] {process_utils.py:189} INFO -     added seed packages: pip==23.1, setuptools==67.6.1, wheel==0.40.0
[2023-07-11T21:47:57.134+0000] {process_utils.py:189} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2023-07-11T21:47:57.188+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venvmaoe874m/bin/pip install -r /tmp/venvmaoe874m/requirements.txt
[2023-07-11T21:47:57.203+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T21:47:57.756+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T21:47:57.757+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/bin/pip", line 5, in <module>
[2023-07-11T21:47:57.757+0000] {process_utils.py:189} INFO -     from pip._internal.cli.main import main
[2023-07-11T21:47:57.758+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/cli/main.py", line 9, in <module>
[2023-07-11T21:47:57.758+0000] {process_utils.py:189} INFO -     from pip._internal.cli.autocompletion import autocomplete
[2023-07-11T21:47:57.758+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py", line 10, in <module>
[2023-07-11T21:47:57.758+0000] {process_utils.py:189} INFO -     from pip._internal.cli.main_parser import create_main_parser
[2023-07-11T21:47:57.758+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py", line 9, in <module>
[2023-07-11T21:47:57.759+0000] {process_utils.py:189} INFO -     from pip._internal.build_env import get_runnable_pip
[2023-07-11T21:47:57.759+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/build_env.py", line 19, in <module>
[2023-07-11T21:47:57.759+0000] {process_utils.py:189} INFO -     from pip._internal.cli.spinners import open_spinner
[2023-07-11T21:47:57.759+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/cli/spinners.py", line 9, in <module>
[2023-07-11T21:47:57.759+0000] {process_utils.py:189} INFO -     from pip._internal.utils.logging import get_indentation
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/utils/logging.py", line 29, in <module>
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -     from pip._internal.utils.misc import ensure_dir
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/utils/misc.py", line 44, in <module>
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -     from pip._internal.locations import get_major_minor_version
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/locations/__init__.py", line 66, in <module>
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -     from . import _distutils
[2023-07-11T21:47:57.760+0000] {process_utils.py:189} INFO -   File "/tmp/venvmaoe874m/lib/python3.9/site-packages/pip/_internal/locations/_distutils.py", line 20, in <module>
[2023-07-11T21:47:57.761+0000] {process_utils.py:189} INFO -     from distutils.cmd import Command as DistutilsCommand
[2023-07-11T21:47:57.761+0000] {process_utils.py:189} INFO - ModuleNotFoundError: No module named 'distutils.cmd'
[2023-07-11T21:47:57.858+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 575, in execute_callable
    pip_install_options=self.pip_install_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/python_virtualenv.py", line 99, in prepare_virtualenv
    execute_in_subprocess(pip_cmd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/tmp/venvmaoe874m/bin/pip', 'install', '-r', '/tmp/venvmaoe874m/requirements.txt']' returned non-zero exit status 1.
[2023-07-11T21:47:57.868+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230710T000000, start_date=20230711T214755, end_date=20230711T214757
[2023-07-11T21:47:57.881+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 4 for task scrap_data (Command '['/tmp/venvmaoe874m/bin/pip', 'install', '-r', '/tmp/venvmaoe874m/requirements.txt']' returned non-zero exit status 1.; 81)
[2023-07-11T21:47:57.910+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T21:47:57.930+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-11T22:34:29.131+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T22:34:29.144+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T22:34:29.144+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T22:34:29.153+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonVirtualenvDecoratedOperator): scrap_data> on 2023-07-10 00:00:00+00:00
[2023-07-11T22:34:29.157+0000] {standard_task_runner.py:57} INFO - Started process 602 to run task
[2023-07-11T22:34:29.159+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmps9ywvdjo']
[2023-07-11T22:34:29.160+0000] {standard_task_runner.py:85} INFO - Job 5: Subtask scrap_data
[2023-07-11T22:34:29.191+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [running]> on host d1838cb5df30
[2023-07-11T22:34:29.247+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-11T22:34:29.248+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python -m virtualenv /tmp/venvkwkk7q9e --system-site-packages --python=python3.9
[2023-07-11T22:34:29.257+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:34:30.844+0000] {process_utils.py:189} INFO - created virtual environment CPython3.9.2.final.0-64 in 1011ms
[2023-07-11T22:34:30.844+0000] {process_utils.py:189} INFO -   creator CPython3Posix(dest=/tmp/venvkwkk7q9e, clear=False, no_vcs_ignore=False, global=True)
[2023-07-11T22:34:30.845+0000] {process_utils.py:189} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/tmp/tmp8exxp91i)
[2023-07-11T22:34:30.845+0000] {process_utils.py:189} INFO -     added seed packages: pip==23.1, setuptools==67.6.1, wheel==0.40.0
[2023-07-11T22:34:30.845+0000] {process_utils.py:189} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2023-07-11T22:34:30.871+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venvkwkk7q9e/bin/pip install -r /tmp/venvkwkk7q9e/requirements.txt
[2023-07-11T22:34:30.878+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:34:31.269+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T22:34:31.270+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/bin/pip", line 5, in <module>
[2023-07-11T22:34:31.270+0000] {process_utils.py:189} INFO -     from pip._internal.cli.main import main
[2023-07-11T22:34:31.270+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/cli/main.py", line 9, in <module>
[2023-07-11T22:34:31.270+0000] {process_utils.py:189} INFO -     from pip._internal.cli.autocompletion import autocomplete
[2023-07-11T22:34:31.270+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py", line 10, in <module>
[2023-07-11T22:34:31.271+0000] {process_utils.py:189} INFO -     from pip._internal.cli.main_parser import create_main_parser
[2023-07-11T22:34:31.271+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py", line 9, in <module>
[2023-07-11T22:34:31.271+0000] {process_utils.py:189} INFO -     from pip._internal.build_env import get_runnable_pip
[2023-07-11T22:34:31.271+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/build_env.py", line 19, in <module>
[2023-07-11T22:34:31.271+0000] {process_utils.py:189} INFO -     from pip._internal.cli.spinners import open_spinner
[2023-07-11T22:34:31.272+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/cli/spinners.py", line 9, in <module>
[2023-07-11T22:34:31.272+0000] {process_utils.py:189} INFO -     from pip._internal.utils.logging import get_indentation
[2023-07-11T22:34:31.272+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/utils/logging.py", line 29, in <module>
[2023-07-11T22:34:31.272+0000] {process_utils.py:189} INFO -     from pip._internal.utils.misc import ensure_dir
[2023-07-11T22:34:31.273+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/utils/misc.py", line 44, in <module>
[2023-07-11T22:34:31.273+0000] {process_utils.py:189} INFO -     from pip._internal.locations import get_major_minor_version
[2023-07-11T22:34:31.273+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/locations/__init__.py", line 66, in <module>
[2023-07-11T22:34:31.273+0000] {process_utils.py:189} INFO -     from . import _distutils
[2023-07-11T22:34:31.273+0000] {process_utils.py:189} INFO -   File "/tmp/venvkwkk7q9e/lib/python3.9/site-packages/pip/_internal/locations/_distutils.py", line 20, in <module>
[2023-07-11T22:34:31.274+0000] {process_utils.py:189} INFO -     from distutils.cmd import Command as DistutilsCommand
[2023-07-11T22:34:31.274+0000] {process_utils.py:189} INFO - ModuleNotFoundError: No module named 'distutils.cmd'
[2023-07-11T22:34:31.322+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 575, in execute_callable
    pip_install_options=self.pip_install_options,
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/python_virtualenv.py", line 99, in prepare_virtualenv
    execute_in_subprocess(pip_cmd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/tmp/venvkwkk7q9e/bin/pip', 'install', '-r', '/tmp/venvkwkk7q9e/requirements.txt']' returned non-zero exit status 1.
[2023-07-11T22:34:31.333+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230710T000000, start_date=20230711T223429, end_date=20230711T223431
[2023-07-11T22:34:31.356+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 5 for task scrap_data (Command '['/tmp/venvkwkk7q9e/bin/pip', 'install', '-r', '/tmp/venvkwkk7q9e/requirements.txt']' returned non-zero exit status 1.; 602)
[2023-07-11T22:34:31.382+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T22:34:31.399+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-11T22:37:03.655+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T22:37:03.662+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T22:37:03.662+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T22:37:03.671+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonVirtualenvDecoratedOperator): scrap_data> on 2023-07-10 00:00:00+00:00
[2023-07-11T22:37:03.675+0000] {standard_task_runner.py:57} INFO - Started process 81 to run task
[2023-07-11T22:37:03.677+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmpbvz9cuh_']
[2023-07-11T22:37:03.677+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask scrap_data
[2023-07-11T22:37:03.712+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [running]> on host c457401c0b6d
[2023-07-11T22:37:03.779+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-11T22:37:03.780+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python -m virtualenv /tmp/venvowkqcq_i --system-site-packages --python=python3.9
[2023-07-11T22:37:03.788+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:37:05.491+0000] {process_utils.py:189} INFO - created virtual environment CPython3.9.2.final.0-64 in 1127ms
[2023-07-11T22:37:05.491+0000] {process_utils.py:189} INFO -   creator CPython3Posix(dest=/tmp/venvowkqcq_i, clear=False, no_vcs_ignore=False, global=True)
[2023-07-11T22:37:05.491+0000] {process_utils.py:189} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/tmp/tmp9iexvdyl)
[2023-07-11T22:37:05.491+0000] {process_utils.py:189} INFO -     added seed packages: pip==23.1, setuptools==67.6.1, wheel==0.40.0
[2023-07-11T22:37:05.491+0000] {process_utils.py:189} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2023-07-11T22:37:05.517+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venvowkqcq_i/bin/pip install -r /tmp/venvowkqcq_i/requirements.txt
[2023-07-11T22:37:05.527+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:37:06.079+0000] {process_utils.py:189} INFO - WARNING: The directory '/home/***/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.
[2023-07-11T22:37:06.266+0000] {process_utils.py:189} INFO - Collecting selenium (from -r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:06.406+0000] {process_utils.py:189} INFO -   Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)
[2023-07-11T22:37:06.587+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 37.8 MB/s eta 0:00:00
[2023-07-11T22:37:06.639+0000] {process_utils.py:189} INFO - Collecting pyspark (from -r /tmp/venvowkqcq_i/requirements.txt (line 2))
[2023-07-11T22:37:06.664+0000] {process_utils.py:189} INFO -   Downloading pyspark-3.4.1.tar.gz (310.8 MB)
[2023-07-11T22:37:11.927+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 310.8/310.8 MB 69.8 MB/s eta 0:00:00
[2023-07-11T22:37:21.696+0000] {process_utils.py:189} INFO -   Preparing metadata (setup.py): started
[2023-07-11T22:37:22.067+0000] {process_utils.py:189} INFO -   Preparing metadata (setup.py): finished with status 'done'
[2023-07-11T22:37:22.120+0000] {process_utils.py:189} INFO - Collecting dateparser (from -r /tmp/venvowkqcq_i/requirements.txt (line 3))
[2023-07-11T22:37:22.146+0000] {process_utils.py:189} INFO -   Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)
[2023-07-11T22:37:22.172+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.8/293.8 kB 186.9 MB/s eta 0:00:00
[2023-07-11T22:37:22.262+0000] {process_utils.py:189} INFO - Collecting urllib3[socks]<3,>=1.26 (from selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:22.283+0000] {process_utils.py:189} INFO -   Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)
[2023-07-11T22:37:22.288+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.6/123.6 kB 41.3 MB/s eta 0:00:00
[2023-07-11T22:37:22.337+0000] {process_utils.py:189} INFO - Collecting trio~=0.17 (from selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:22.365+0000] {process_utils.py:189} INFO -   Downloading trio-0.22.1-py3-none-any.whl (399 kB)
[2023-07-11T22:37:22.379+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 399.3/399.3 kB 33.6 MB/s eta 0:00:00
[2023-07-11T22:37:22.420+0000] {process_utils.py:189} INFO - Collecting trio-websocket~=0.9 (from selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:22.444+0000] {process_utils.py:189} INFO -   Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)
[2023-07-11T22:37:22.491+0000] {process_utils.py:189} INFO - Collecting certifi>=2021.10.8 (from selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:22.520+0000] {process_utils.py:189} INFO -   Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)
[2023-07-11T22:37:22.528+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.0/157.0 kB 27.7 MB/s eta 0:00:00
[2023-07-11T22:37:22.587+0000] {process_utils.py:189} INFO - Collecting py4j==0.10.9.7 (from pyspark->-r /tmp/venvowkqcq_i/requirements.txt (line 2))
[2023-07-11T22:37:22.615+0000] {process_utils.py:189} INFO -   Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)
[2023-07-11T22:37:22.665+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 kB 4.0 MB/s eta 0:00:00
[2023-07-11T22:37:22.718+0000] {process_utils.py:189} INFO - Collecting python-dateutil (from dateparser->-r /tmp/venvowkqcq_i/requirements.txt (line 3))
[2023-07-11T22:37:22.744+0000] {process_utils.py:189} INFO -   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
[2023-07-11T22:37:22.757+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 23.6 MB/s eta 0:00:00
[2023-07-11T22:37:22.865+0000] {process_utils.py:189} INFO - Collecting pytz (from dateparser->-r /tmp/venvowkqcq_i/requirements.txt (line 3))
[2023-07-11T22:37:22.888+0000] {process_utils.py:189} INFO -   Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)
[2023-07-11T22:37:22.901+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.3/502.3 kB 44.9 MB/s eta 0:00:00
[2023-07-11T22:37:23.429+0000] {process_utils.py:189} INFO - Collecting regex!=2019.02.19,!=2021.8.27 (from dateparser->-r /tmp/venvowkqcq_i/requirements.txt (line 3))
[2023-07-11T22:37:23.453+0000] {process_utils.py:189} INFO -   Downloading regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)
[2023-07-11T22:37:23.472+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.9/769.9 kB 47.8 MB/s eta 0:00:00
[2023-07-11T22:37:23.523+0000] {process_utils.py:189} INFO - Collecting tzlocal (from dateparser->-r /tmp/venvowkqcq_i/requirements.txt (line 3))
[2023-07-11T22:37:23.547+0000] {process_utils.py:189} INFO -   Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)
[2023-07-11T22:37:23.619+0000] {process_utils.py:189} INFO - Collecting attrs>=20.1.0 (from trio~=0.17->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:23.642+0000] {process_utils.py:189} INFO -   Downloading attrs-23.1.0-py3-none-any.whl (61 kB)
[2023-07-11T22:37:23.650+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 14.0 MB/s eta 0:00:00
[2023-07-11T22:37:23.892+0000] {process_utils.py:189} INFO - Collecting sortedcontainers (from trio~=0.17->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:23.914+0000] {process_utils.py:189} INFO -   Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
[2023-07-11T22:37:23.949+0000] {process_utils.py:189} INFO - Collecting idna (from trio~=0.17->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:23.969+0000] {process_utils.py:189} INFO -   Downloading idna-3.4-py3-none-any.whl (61 kB)
[2023-07-11T22:37:23.973+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 33.5 MB/s eta 0:00:00
[2023-07-11T22:37:24.008+0000] {process_utils.py:189} INFO - Collecting outcome (from trio~=0.17->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:24.030+0000] {process_utils.py:189} INFO -   Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)
[2023-07-11T22:37:24.101+0000] {process_utils.py:189} INFO - Collecting sniffio (from trio~=0.17->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:24.125+0000] {process_utils.py:189} INFO -   Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)
[2023-07-11T22:37:24.169+0000] {process_utils.py:189} INFO - Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:24.191+0000] {process_utils.py:189} INFO -   Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)
[2023-07-11T22:37:24.236+0000] {process_utils.py:189} INFO - Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:24.258+0000] {process_utils.py:189} INFO -   Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)
[2023-07-11T22:37:24.309+0000] {process_utils.py:189} INFO - Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:24.330+0000] {process_utils.py:189} INFO -   Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)
[2023-07-11T22:37:24.383+0000] {process_utils.py:189} INFO - Collecting six>=1.5 (from python-dateutil->dateparser->-r /tmp/venvowkqcq_i/requirements.txt (line 3))
[2023-07-11T22:37:24.405+0000] {process_utils.py:189} INFO -   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
[2023-07-11T22:37:24.482+0000] {process_utils.py:189} INFO - Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r /tmp/venvowkqcq_i/requirements.txt (line 1))
[2023-07-11T22:37:24.505+0000] {process_utils.py:189} INFO -   Downloading h11-0.14.0-py3-none-any.whl (58 kB)
[2023-07-11T22:37:24.512+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 25.8 MB/s eta 0:00:00
[2023-07-11T22:37:24.534+0000] {process_utils.py:189} INFO - Building wheels for collected packages: pyspark
[2023-07-11T22:37:24.540+0000] {process_utils.py:189} INFO -   Building wheel for pyspark (setup.py): started
[2023-07-11T22:37:40.523+0000] {process_utils.py:189} INFO -   Building wheel for pyspark (setup.py): finished with status 'done'
[2023-07-11T22:37:40.785+0000] {process_utils.py:189} INFO -   Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=80ecbd7d04723930d4e49bbeac627a9e3edb84416a2d1d623219f21a8b387027
[2023-07-11T22:37:40.785+0000] {process_utils.py:189} INFO -   Stored in directory: /tmp/pip-ephem-wheel-cache-eygeogut/wheels/2b/9a/39/d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8
[2023-07-11T22:37:40.791+0000] {process_utils.py:189} INFO - Successfully built pyspark
[2023-07-11T22:37:40.877+0000] {process_utils.py:189} INFO - Installing collected packages: sortedcontainers, pytz, py4j, urllib3, tzlocal, sniffio, six, regex, pyspark, pysocks, idna, h11, exceptiongroup, certifi, attrs, wsproto, python-dateutil, outcome, trio, dateparser, trio-websocket, selenium
[2023-07-11T22:37:47.168+0000] {process_utils.py:189} INFO - Successfully installed attrs-23.1.0 certifi-2023.5.7 dateparser-1.1.8 exceptiongroup-1.1.2 h11-0.14.0 idna-3.4 outcome-1.2.0 py4j-0.10.9.7 pysocks-1.7.1 pyspark-3.4.1 python-dateutil-2.8.2 pytz-2023.3 regex-2023.6.3 selenium-4.10.0 six-1.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.22.1 trio-websocket-0.10.3 tzlocal-5.0.1 urllib3-2.0.3 wsproto-1.2.0
[2023-07-11T22:37:47.315+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:37:47.316+0000] {process_utils.py:189} INFO - [notice] A new release of pip is available: 23.1 -> 23.1.2
[2023-07-11T22:37:47.316+0000] {process_utils.py:189} INFO - [notice] To update, run: /tmp/venvowkqcq_i/bin/python -m pip install --upgrade pip
[2023-07-11T22:37:47.542+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venvowkqcq_i/bin/python /tmp/venvowkqcq_i/script.py /tmp/venvowkqcq_i/script.in /tmp/venvowkqcq_i/script.out /tmp/venvowkqcq_i/string_args.txt
[2023-07-11T22:37:47.552+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:37:47.875+0000] {process_utils.py:189} INFO - /tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-11T22:37:50.092+0000] {process_utils.py:189} INFO - Setting default log level to "WARN".
[2023-07-11T22:37:50.092+0000] {process_utils.py:189} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2023-07-11T22:37:50.350+0000] {process_utils.py:189} INFO - 23/07/11 22:37:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-11T22:38:04.524+0000] {process_utils.py:189} INFO - [Stage 0:>                                                          (0 + 1) / 1]23/07/11 22:38:04 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2023-07-11T22:38:04.524+0000] {process_utils.py:189} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:38:04.525+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:38:04.525+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:38:04.525+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:38:04.525+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:04.525+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.526+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:38:04.527+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:38:04.528+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:38:04.529+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:38:04.529+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:38:04.529+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:38:04.544+0000] {process_utils.py:189} INFO - 23/07/11 22:38:04 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (c457401c0b6d executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:38:04.544+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:38:04.544+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:38:04.545+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:38:04.545+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:04.545+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:38:04.545+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:38:04.546+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:38:04.546+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:38:04.546+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:38:04.546+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:38:04.547+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:38:04.548+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:38:04.549+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:38:04.549+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:04.549+0000] {process_utils.py:189} INFO - 23/07/11 22:38:04 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2023-07-11T22:38:04.793+0000] {process_utils.py:189} INFO - 1) BROWSER OK
[2023-07-11T22:38:04.793+0000] {process_utils.py:189} INFO - 2) GET OK
[2023-07-11T22:38:04.793+0000] {process_utils.py:189} INFO - 3) BUTTON OK
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - 4) CLICK OK
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - 5) FETCH OK
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - 6) FETCH 2 OK
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - Loading data...
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.794+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.795+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO - INSEPECTING ELEMENT
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/script.py", line 160, in <module>
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO -     res = scrapData(*arg_dict["args"], **arg_dict["kwargs"])
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/script.py", line 155, in scrapData
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO -     data_df.show()
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 899, in show
[2023-07-11T22:38:04.796+0000] {process_utils.py:189} INFO -     print(self._jdf.showString(n, 20, vertical))
[2023-07-11T22:38:04.797+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2023-07-11T22:38:04.797+0000] {process_utils.py:189} INFO -     return_value = get_return_value(
[2023-07-11T22:38:04.797+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
[2023-07-11T22:38:04.797+0000] {process_utils.py:189} INFO -     return f(*a, **kw)
[2023-07-11T22:38:04.797+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/py4j/protocol.py", line 326, in get_return_value
[2023-07-11T22:38:04.797+0000] {process_utils.py:189} INFO -     raise Py4JJavaError(
[2023-07-11T22:38:04.839+0000] {process_utils.py:189} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o38.showString.
[2023-07-11T22:38:04.839+0000] {process_utils.py:189} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (c457401c0b6d executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:38:04.840+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:38:04.840+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:38:04.840+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:38:04.841+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:04.841+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:38:04.841+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:38:04.841+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:38:04.841+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:38:04.842+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:38:04.842+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:38:04.842+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.842+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.843+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:38:04.843+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:38:04.843+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:38:04.843+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:38:04.843+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:38:04.844+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:38:04.844+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:38:04.844+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:38:04.844+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:38:04.844+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:38:04.845+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:38:04.845+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:38:04.845+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:38:04.845+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:38:04.845+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:38:04.846+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:38:04.846+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:38:04.846+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:38:04.846+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:04.847+0000] {process_utils.py:189} INFO - Driver stacktrace:
[2023-07-11T22:38:04.847+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
[2023-07-11T22:38:04.847+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
[2023-07-11T22:38:04.847+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
[2023-07-11T22:38:04.847+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at scala.Option.foreach(Option.scala:407)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
[2023-07-11T22:38:04.848+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
[2023-07-11T22:38:04.849+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
[2023-07-11T22:38:04.850+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-07-11T22:38:04.851+0000] {process_utils.py:189} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO -   File "/tmp/venvowkqcq_i/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:04.852+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:38:04.853+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:38:04.854+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:38:04.855+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:38:04.855+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:38:04.855+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:38:04.855+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:38:04.855+0000] {process_utils.py:189} INFO - 	... 1 more
[2023-07-11T22:38:04.855+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:38:05.430+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 578, in execute_callable
    result = self._execute_python_callable_in_subprocess(python_path, tmp_path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 434, in _execute_python_callable_in_subprocess
    os.fspath(string_args_path),
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/tmp/venvowkqcq_i/bin/python', '/tmp/venvowkqcq_i/script.py', '/tmp/venvowkqcq_i/script.in', '/tmp/venvowkqcq_i/script.out', '/tmp/venvowkqcq_i/string_args.txt']' returned non-zero exit status 1.
[2023-07-11T22:38:05.440+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230710T000000, start_date=20230711T223703, end_date=20230711T223805
[2023-07-11T22:38:05.452+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 4 for task scrap_data (Command '['/tmp/venvowkqcq_i/bin/python', '/tmp/venvowkqcq_i/script.py', '/tmp/venvowkqcq_i/script.in', '/tmp/venvowkqcq_i/script.out', '/tmp/venvowkqcq_i/string_args.txt']' returned non-zero exit status 1.; 81)
[2023-07-11T22:38:05.474+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T22:38:05.493+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2023-07-11T22:46:21.845+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T22:46:21.852+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [queued]>
[2023-07-11T22:46:21.852+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-11T22:46:21.864+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonVirtualenvDecoratedOperator): scrap_data> on 2023-07-10 00:00:00+00:00
[2023-07-11T22:46:21.871+0000] {standard_task_runner.py:57} INFO - Started process 81 to run task
[2023-07-11T22:46:21.873+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_scrap_biobio_v14', 'scrap_data', 'scheduled__2023-07-10T00:00:00+00:00', '--job-id', '4', '--raw', '--subdir', 'DAGS_FOLDER/dag_scrap_biobio.py', '--cfg-path', '/tmp/tmpg5gse_k9']
[2023-07-11T22:46:21.874+0000] {standard_task_runner.py:85} INFO - Job 4: Subtask scrap_data
[2023-07-11T22:46:21.906+0000] {task_command.py:410} INFO - Running <TaskInstance: dag_scrap_biobio_v14.scrap_data scheduled__2023-07-10T00:00:00+00:00 [running]> on host 5a2aa5f79b19
[2023-07-11T22:46:21.968+0000] {taskinstance.py:1547} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='dag_scrap_biobio_v14' AIRFLOW_CTX_TASK_ID='scrap_data' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2023-07-10T00:00:00+00:00'
[2023-07-11T22:46:21.969+0000] {process_utils.py:181} INFO - Executing cmd: /usr/local/bin/python -m virtualenv /tmp/venvtqnyfpl9 --system-site-packages --python=python3.9
[2023-07-11T22:46:21.979+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:46:23.642+0000] {process_utils.py:189} INFO - created virtual environment CPython3.9.2.final.0-64 in 1086ms
[2023-07-11T22:46:23.643+0000] {process_utils.py:189} INFO -   creator CPython3Posix(dest=/tmp/venvtqnyfpl9, clear=False, no_vcs_ignore=False, global=True)
[2023-07-11T22:46:23.643+0000] {process_utils.py:189} INFO -   seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/tmp/tmpy1ng6rsk)
[2023-07-11T22:46:23.643+0000] {process_utils.py:189} INFO -     added seed packages: pip==23.1, setuptools==67.6.1, wheel==0.40.0
[2023-07-11T22:46:23.643+0000] {process_utils.py:189} INFO -   activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator
[2023-07-11T22:46:23.684+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venvtqnyfpl9/bin/pip install -r /tmp/venvtqnyfpl9/requirements.txt
[2023-07-11T22:46:23.692+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:46:24.225+0000] {process_utils.py:189} INFO - WARNING: The directory '/home/***/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.
[2023-07-11T22:46:24.411+0000] {process_utils.py:189} INFO - Collecting selenium (from -r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:24.554+0000] {process_utils.py:189} INFO -   Downloading selenium-4.10.0-py3-none-any.whl (6.7 MB)
[2023-07-11T22:46:24.731+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.7/6.7 MB 38.5 MB/s eta 0:00:00
[2023-07-11T22:46:24.777+0000] {process_utils.py:189} INFO - Collecting pyspark (from -r /tmp/venvtqnyfpl9/requirements.txt (line 2))
[2023-07-11T22:46:24.807+0000] {process_utils.py:189} INFO -   Downloading pyspark-3.4.1.tar.gz (310.8 MB)
[2023-07-11T22:46:32.257+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 310.8/310.8 MB 63.3 MB/s eta 0:00:00
[2023-07-11T22:46:35.911+0000] {process_utils.py:189} INFO -   Preparing metadata (setup.py): started
[2023-07-11T22:46:36.123+0000] {process_utils.py:189} INFO -   Preparing metadata (setup.py): finished with status 'done'
[2023-07-11T22:46:36.170+0000] {process_utils.py:189} INFO - Collecting dateparser (from -r /tmp/venvtqnyfpl9/requirements.txt (line 3))
[2023-07-11T22:46:36.196+0000] {process_utils.py:189} INFO -   Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)
[2023-07-11T22:46:36.219+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 293.8/293.8 kB 321.4 MB/s eta 0:00:00
[2023-07-11T22:46:36.278+0000] {process_utils.py:189} INFO - Collecting urllib3[socks]<3,>=1.26 (from selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:36.300+0000] {process_utils.py:189} INFO -   Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)
[2023-07-11T22:46:36.305+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.6/123.6 kB 33.4 MB/s eta 0:00:00
[2023-07-11T22:46:36.344+0000] {process_utils.py:189} INFO - Collecting trio~=0.17 (from selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:36.366+0000] {process_utils.py:189} INFO -   Downloading trio-0.22.1-py3-none-any.whl (399 kB)
[2023-07-11T22:46:36.380+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 399.3/399.3 kB 32.8 MB/s eta 0:00:00
[2023-07-11T22:46:36.413+0000] {process_utils.py:189} INFO - Collecting trio-websocket~=0.9 (from selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:36.437+0000] {process_utils.py:189} INFO -   Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)
[2023-07-11T22:46:36.501+0000] {process_utils.py:189} INFO - Collecting certifi>=2021.10.8 (from selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:36.524+0000] {process_utils.py:189} INFO -   Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)
[2023-07-11T22:46:36.532+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.0/157.0 kB 34.7 MB/s eta 0:00:00
[2023-07-11T22:46:36.574+0000] {process_utils.py:189} INFO - Collecting py4j==0.10.9.7 (from pyspark->-r /tmp/venvtqnyfpl9/requirements.txt (line 2))
[2023-07-11T22:46:36.597+0000] {process_utils.py:189} INFO -   Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)
[2023-07-11T22:46:36.606+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 kB 30.8 MB/s eta 0:00:00
[2023-07-11T22:46:36.769+0000] {process_utils.py:189} INFO - Collecting python-dateutil (from dateparser->-r /tmp/venvtqnyfpl9/requirements.txt (line 3))
[2023-07-11T22:46:36.791+0000] {process_utils.py:189} INFO -   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
[2023-07-11T22:46:36.804+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 23.4 MB/s eta 0:00:00
[2023-07-11T22:46:36.894+0000] {process_utils.py:189} INFO - Collecting pytz (from dateparser->-r /tmp/venvtqnyfpl9/requirements.txt (line 3))
[2023-07-11T22:46:36.915+0000] {process_utils.py:189} INFO -   Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)
[2023-07-11T22:46:36.928+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.3/502.3 kB 46.6 MB/s eta 0:00:00
[2023-07-11T22:46:37.452+0000] {process_utils.py:189} INFO - Collecting regex!=2019.02.19,!=2021.8.27 (from dateparser->-r /tmp/venvtqnyfpl9/requirements.txt (line 3))
[2023-07-11T22:46:37.477+0000] {process_utils.py:189} INFO -   Downloading regex-2023.6.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)
[2023-07-11T22:46:37.496+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.9/769.9 kB 42.1 MB/s eta 0:00:00
[2023-07-11T22:46:37.538+0000] {process_utils.py:189} INFO - Collecting tzlocal (from dateparser->-r /tmp/venvtqnyfpl9/requirements.txt (line 3))
[2023-07-11T22:46:37.566+0000] {process_utils.py:189} INFO -   Downloading tzlocal-5.0.1-py3-none-any.whl (20 kB)
[2023-07-11T22:46:37.613+0000] {process_utils.py:189} INFO - Collecting attrs>=20.1.0 (from trio~=0.17->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:37.636+0000] {process_utils.py:189} INFO -   Downloading attrs-23.1.0-py3-none-any.whl (61 kB)
[2023-07-11T22:46:37.638+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 53.6 MB/s eta 0:00:00
[2023-07-11T22:46:37.671+0000] {process_utils.py:189} INFO - Collecting sortedcontainers (from trio~=0.17->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:37.694+0000] {process_utils.py:189} INFO -   Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)
[2023-07-11T22:46:37.729+0000] {process_utils.py:189} INFO - Collecting idna (from trio~=0.17->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:37.752+0000] {process_utils.py:189} INFO -   Downloading idna-3.4-py3-none-any.whl (61 kB)
[2023-07-11T22:46:37.758+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 24.9 MB/s eta 0:00:00
[2023-07-11T22:46:37.814+0000] {process_utils.py:189} INFO - Collecting outcome (from trio~=0.17->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:37.845+0000] {process_utils.py:189} INFO -   Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)
[2023-07-11T22:46:37.879+0000] {process_utils.py:189} INFO - Collecting sniffio (from trio~=0.17->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:37.903+0000] {process_utils.py:189} INFO -   Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)
[2023-07-11T22:46:37.951+0000] {process_utils.py:189} INFO - Collecting exceptiongroup>=1.0.0rc9 (from trio~=0.17->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:37.973+0000] {process_utils.py:189} INFO -   Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)
[2023-07-11T22:46:38.011+0000] {process_utils.py:189} INFO - Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:38.046+0000] {process_utils.py:189} INFO -   Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)
[2023-07-11T22:46:38.095+0000] {process_utils.py:189} INFO - Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:38.116+0000] {process_utils.py:189} INFO -   Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)
[2023-07-11T22:46:38.158+0000] {process_utils.py:189} INFO - Collecting six>=1.5 (from python-dateutil->dateparser->-r /tmp/venvtqnyfpl9/requirements.txt (line 3))
[2023-07-11T22:46:38.180+0000] {process_utils.py:189} INFO -   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)
[2023-07-11T22:46:38.247+0000] {process_utils.py:189} INFO - Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium->-r /tmp/venvtqnyfpl9/requirements.txt (line 1))
[2023-07-11T22:46:38.268+0000] {process_utils.py:189} INFO -   Downloading h11-0.14.0-py3-none-any.whl (58 kB)
[2023-07-11T22:46:38.273+0000] {process_utils.py:189} INFO -      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 19.3 MB/s eta 0:00:00
[2023-07-11T22:46:38.295+0000] {process_utils.py:189} INFO - Building wheels for collected packages: pyspark
[2023-07-11T22:46:38.296+0000] {process_utils.py:189} INFO -   Building wheel for pyspark (setup.py): started
[2023-07-11T22:46:58.410+0000] {process_utils.py:189} INFO -   Building wheel for pyspark (setup.py): finished with status 'done'
[2023-07-11T22:46:58.590+0000] {process_utils.py:189} INFO -   Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=dd60468b5aabb3592de40ba9c71fb183f68e9c99203414190e288be937c24293
[2023-07-11T22:46:58.591+0000] {process_utils.py:189} INFO -   Stored in directory: /tmp/pip-ephem-wheel-cache-hcupjz3g/wheels/2b/9a/39/d8019ffbfb76a39433455e3d5799e94d3e3cae8f41229f6bf8
[2023-07-11T22:46:58.610+0000] {process_utils.py:189} INFO - Successfully built pyspark
[2023-07-11T22:46:58.697+0000] {process_utils.py:189} INFO - Installing collected packages: sortedcontainers, pytz, py4j, urllib3, tzlocal, sniffio, six, regex, pyspark, pysocks, idna, h11, exceptiongroup, certifi, attrs, wsproto, python-dateutil, outcome, trio, dateparser, trio-websocket, selenium
[2023-07-11T22:47:04.313+0000] {process_utils.py:189} INFO - Successfully installed attrs-23.1.0 certifi-2023.5.7 dateparser-1.1.8 exceptiongroup-1.1.2 h11-0.14.0 idna-3.4 outcome-1.2.0 py4j-0.10.9.7 pysocks-1.7.1 pyspark-3.4.1 python-dateutil-2.8.2 pytz-2023.3 regex-2023.6.3 selenium-4.10.0 six-1.16.0 sniffio-1.3.0 sortedcontainers-2.4.0 trio-0.22.1 trio-websocket-0.10.3 tzlocal-5.0.1 urllib3-2.0.3 wsproto-1.2.0
[2023-07-11T22:47:04.459+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:04.459+0000] {process_utils.py:189} INFO - [notice] A new release of pip is available: 23.1 -> 23.1.2
[2023-07-11T22:47:04.459+0000] {process_utils.py:189} INFO - [notice] To update, run: /tmp/venvtqnyfpl9/bin/python -m pip install --upgrade pip
[2023-07-11T22:47:04.690+0000] {process_utils.py:181} INFO - Executing cmd: /tmp/venvtqnyfpl9/bin/python /tmp/venvtqnyfpl9/script.py /tmp/venvtqnyfpl9/script.in /tmp/venvtqnyfpl9/script.out /tmp/venvtqnyfpl9/string_args.txt
[2023-07-11T22:47:04.701+0000] {process_utils.py:185} INFO - Output:
[2023-07-11T22:47:05.036+0000] {process_utils.py:189} INFO - /tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2023-07-11T22:47:06.550+0000] {process_utils.py:189} INFO - Setting default log level to "WARN".
[2023-07-11T22:47:06.551+0000] {process_utils.py:189} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2023-07-11T22:47:06.728+0000] {process_utils.py:189} INFO - 23/07/11 22:47:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2023-07-11T22:47:17.785+0000] {process_utils.py:189} INFO - [Stage 0:>                                                          (0 + 1) / 1]23/07/11 22:47:17 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2023-07-11T22:47:17.785+0000] {process_utils.py:189} INFO - org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:47:17.785+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:47:17.785+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:47:17.786+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:47:17.786+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:17.786+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:47:17.786+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:47:17.786+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:47:17.787+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:47:17.787+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:47:17.787+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:47:17.787+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:17.787+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:17.787+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:47:17.788+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:47:17.788+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:47:17.788+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:47:17.788+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:47:17.788+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:47:17.788+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:47:17.789+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:47:17.790+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:47:17.790+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:47:17.790+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:47:17.813+0000] {process_utils.py:189} INFO - 23/07/11 22:47:17 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (5a2aa5f79b19 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:47:17.814+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:47:17.815+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:47:17.816+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:47:17.817+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:47:17.817+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:47:17.817+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:47:17.817+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:47:17.817+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:47:17.817+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:47:17.818+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:47:17.818+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:47:17.818+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:47:17.818+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:17.818+0000] {process_utils.py:189} INFO - 23/07/11 22:47:17 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2023-07-11T22:47:18.117+0000] {process_utils.py:189} INFO - 1) BROWSER OK
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - 2) GET OK
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - 3) BUTTON OK
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - 4) CLICK OK
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - 5) FETCH OK
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - 6) FETCH 2 OK
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - Loading data...
[2023-07-11T22:47:18.118+0000] {process_utils.py:189} INFO - Traceback (most recent call last):
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/script.py", line 159, in <module>
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -     res = scrapData(*arg_dict["args"], **arg_dict["kwargs"])
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/script.py", line 154, in scrapData
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -     data_df.show()
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/sql/dataframe.py", line 899, in show
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -     print(self._jdf.showString(n, 20, vertical))
[2023-07-11T22:47:18.119+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2023-07-11T22:47:18.120+0000] {process_utils.py:189} INFO -     return_value = get_return_value(
[2023-07-11T22:47:18.120+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py", line 169, in deco
[2023-07-11T22:47:18.120+0000] {process_utils.py:189} INFO -     return f(*a, **kw)
[2023-07-11T22:47:18.120+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/py4j/protocol.py", line 326, in get_return_value
[2023-07-11T22:47:18.120+0000] {process_utils.py:189} INFO -     raise Py4JJavaError(
[2023-07-11T22:47:18.167+0000] {process_utils.py:189} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o38.showString.
[2023-07-11T22:47:18.167+0000] {process_utils.py:189} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (5a2aa5f79b19 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:47:18.167+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:47:18.168+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:47:18.168+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:47:18.168+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:18.168+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:47:18.168+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:18.169+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:47:18.170+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:47:18.170+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:47:18.170+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:47:18.170+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:47:18.170+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:47:18.170+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:47:18.171+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:47:18.171+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:47:18.171+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:47:18.171+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:47:18.171+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:47:18.171+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:18.172+0000] {process_utils.py:189} INFO - Driver stacktrace:
[2023-07-11T22:47:18.173+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
[2023-07-11T22:47:18.173+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
[2023-07-11T22:47:18.173+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
[2023-07-11T22:47:18.173+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2023-07-11T22:47:18.173+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2023-07-11T22:47:18.173+0000] {process_utils.py:189} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2023-07-11T22:47:18.174+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
[2023-07-11T22:47:18.174+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
[2023-07-11T22:47:18.174+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
[2023-07-11T22:47:18.174+0000] {process_utils.py:189} INFO - 	at scala.Option.foreach(Option.scala:407)
[2023-07-11T22:47:18.174+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
[2023-07-11T22:47:18.175+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
[2023-07-11T22:47:18.175+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
[2023-07-11T22:47:18.175+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
[2023-07-11T22:47:18.175+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2023-07-11T22:47:18.175+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
[2023-07-11T22:47:18.175+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
[2023-07-11T22:47:18.176+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
[2023-07-11T22:47:18.177+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
[2023-07-11T22:47:18.177+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
[2023-07-11T22:47:18.177+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
[2023-07-11T22:47:18.177+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
[2023-07-11T22:47:18.177+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
[2023-07-11T22:47:18.177+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
[2023-07-11T22:47:18.178+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
[2023-07-11T22:47:18.178+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
[2023-07-11T22:47:18.178+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
[2023-07-11T22:47:18.178+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
[2023-07-11T22:47:18.178+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
[2023-07-11T22:47:18.178+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
[2023-07-11T22:47:18.179+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
[2023-07-11T22:47:18.179+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2023-07-11T22:47:18.179+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[2023-07-11T22:47:18.179+0000] {process_utils.py:189} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2023-07-11T22:47:18.179+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
[2023-07-11T22:47:18.179+0000] {process_utils.py:189} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2023-07-11T22:47:18.180+0000] {process_utils.py:189} INFO - 	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-11T22:47:18.181+0000] {process_utils.py:189} INFO - Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
[2023-07-11T22:47:18.181+0000] {process_utils.py:189} INFO -   File "/tmp/venvtqnyfpl9/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py", line 690, in main
[2023-07-11T22:47:18.181+0000] {process_utils.py:189} INFO -     % ("%d.%d" % sys.version_info[:2], version)
[2023-07-11T22:47:18.181+0000] {process_utils.py:189} INFO - RuntimeError: Python in worker has different version 3.7 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.
[2023-07-11T22:47:18.181+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:18.182+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)
[2023-07-11T22:47:18.182+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)
[2023-07-11T22:47:18.182+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)
[2023-07-11T22:47:18.182+0000] {process_utils.py:189} INFO - 	at org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)
[2023-07-11T22:47:18.182+0000] {process_utils.py:189} INFO - 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
[2023-07-11T22:47:18.182+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)
[2023-07-11T22:47:18.183+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:18.183+0000] {process_utils.py:189} INFO - 	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2023-07-11T22:47:18.183+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2023-07-11T22:47:18.183+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2023-07-11T22:47:18.183+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
[2023-07-11T22:47:18.183+0000] {process_utils.py:189} INFO - 	at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
[2023-07-11T22:47:18.184+0000] {process_utils.py:189} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
[2023-07-11T22:47:18.185+0000] {process_utils.py:189} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:139)
[2023-07-11T22:47:18.185+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
[2023-07-11T22:47:18.185+0000] {process_utils.py:189} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
[2023-07-11T22:47:18.185+0000] {process_utils.py:189} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
[2023-07-11T22:47:18.185+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
[2023-07-11T22:47:18.185+0000] {process_utils.py:189} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
[2023-07-11T22:47:18.186+0000] {process_utils.py:189} INFO - 	... 1 more
[2023-07-11T22:47:18.186+0000] {process_utils.py:189} INFO - 
[2023-07-11T22:47:18.756+0000] {taskinstance.py:1824} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/decorators/base.py", line 220, in execute
    return_value = super().execute(context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 374, in execute
    return super().execute(context=serializable_context)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 181, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 578, in execute_callable
    result = self._execute_python_callable_in_subprocess(python_path, tmp_path)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/operators/python.py", line 434, in _execute_python_callable_in_subprocess
    os.fspath(string_args_path),
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 170, in execute_in_subprocess
    execute_in_subprocess_with_kwargs(cmd, cwd=cwd)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/process_utils.py", line 193, in execute_in_subprocess_with_kwargs
    raise subprocess.CalledProcessError(exit_code, cmd)
subprocess.CalledProcessError: Command '['/tmp/venvtqnyfpl9/bin/python', '/tmp/venvtqnyfpl9/script.py', '/tmp/venvtqnyfpl9/script.in', '/tmp/venvtqnyfpl9/script.out', '/tmp/venvtqnyfpl9/string_args.txt']' returned non-zero exit status 1.
[2023-07-11T22:47:18.766+0000] {taskinstance.py:1350} INFO - Marking task as FAILED. dag_id=dag_scrap_biobio_v14, task_id=scrap_data, execution_date=20230710T000000, start_date=20230711T224621, end_date=20230711T224718
[2023-07-11T22:47:18.777+0000] {standard_task_runner.py:109} ERROR - Failed to execute job 4 for task scrap_data (Command '['/tmp/venvtqnyfpl9/bin/python', '/tmp/venvtqnyfpl9/script.py', '/tmp/venvtqnyfpl9/script.in', '/tmp/venvtqnyfpl9/script.out', '/tmp/venvtqnyfpl9/string_args.txt']' returned non-zero exit status 1.; 81)
[2023-07-11T22:47:18.804+0000] {local_task_job_runner.py:225} INFO - Task exited with return code 1
[2023-07-11T22:47:18.818+0000] {taskinstance.py:2653} INFO - 0 downstream tasks scheduled from follow-on schedule check
