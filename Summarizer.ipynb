{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4169bdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a513260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n",
      "Some weights of EncoderDecoderModel were not initialized from the model checkpoint at mrm8488/bert2bert_shared-spanish-finetuned-summarization and are newly initialized: ['decoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.attention.self.value.bias', 'decoder.cls.predictions.decoder.bias', 'decoder.bert.encoder.layer.10.output.dense.bias', 'decoder.bert.encoder.layer.5.output.dense.bias', 'decoder.bert.encoder.layer.3.attention.output.dense.weight', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.attention.output.dense.bias', 'decoder.bert.encoder.layer.8.attention.output.dense.weight', 'decoder.bert.encoder.layer.5.output.dense.weight', 'decoder.bert.encoder.layer.6.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.intermediate.dense.weight', 'decoder.bert.encoder.layer.1.output.dense.weight', 'decoder.bert.encoder.layer.1.attention.self.value.bias', 'decoder.bert.encoder.layer.11.output.dense.bias', 'decoder.bert.encoder.layer.2.intermediate.dense.weight', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.self.query.weight', 'decoder.bert.encoder.layer.4.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.attention.self.key.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.6.intermediate.dense.bias', 'decoder.bert.encoder.layer.2.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.attention.self.query.weight', 'decoder.bert.encoder.layer.6.output.LayerNorm.weight', 'decoder.bert.embeddings.LayerNorm.bias', 'decoder.bert.encoder.layer.2.attention.self.value.bias', 'decoder.bert.encoder.layer.4.output.LayerNorm.weight', 'decoder.bert.encoder.layer.7.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.query.bias', 'decoder.bert.encoder.layer.0.attention.self.query.weight', 'decoder.bert.encoder.layer.4.attention.self.key.weight', 'decoder.bert.encoder.layer.7.output.dense.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.attention.self.key.bias', 'decoder.bert.encoder.layer.10.attention.self.key.weight', 'decoder.bert.encoder.layer.0.output.dense.weight', 'decoder.bert.encoder.layer.10.attention.self.query.bias', 'decoder.bert.encoder.layer.9.intermediate.dense.bias', 'decoder.bert.encoder.layer.8.output.dense.weight', 'decoder.bert.encoder.layer.6.output.dense.weight', 'decoder.bert.encoder.layer.8.attention.self.query.bias', 'decoder.bert.encoder.layer.4.attention.self.query.weight', 'decoder.bert.encoder.layer.7.attention.self.key.bias', 'decoder.bert.encoder.layer.0.intermediate.dense.weight', 'decoder.bert.encoder.layer.6.intermediate.dense.weight', 'decoder.bert.encoder.layer.11.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.self.value.weight', 'decoder.bert.encoder.layer.1.output.LayerNorm.weight', 'decoder.bert.embeddings.position_embeddings.weight', 'decoder.bert.encoder.layer.7.attention.output.dense.bias', 'decoder.bert.encoder.layer.0.output.LayerNorm.weight', 'decoder.bert.encoder.layer.5.attention.self.value.weight', 'decoder.bert.encoder.layer.5.intermediate.dense.bias', 'decoder.bert.encoder.layer.5.attention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.self.key.bias', 'decoder.bert.encoder.layer.1.intermediate.dense.bias', 'decoder.bert.encoder.layer.2.attention.self.key.bias', 'decoder.bert.encoder.layer.1.attention.output.dense.bias', 'decoder.bert.encoder.layer.10.attention.self.value.weight', 'decoder.bert.encoder.layer.2.attention.self.value.weight', 'decoder.bert.encoder.layer.5.attention.self.query.weight', 'decoder.bert.encoder.layer.2.attention.self.query.bias', 'decoder.bert.encoder.layer.0.attention.self.value.bias', 'decoder.bert.encoder.layer.0.attention.output.dense.weight', 'decoder.bert.encoder.layer.6.attention.output.dense.weight', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.attention.output.dense.weight', 'decoder.bert.encoder.layer.10.attention.self.query.weight', 'decoder.bert.encoder.layer.10.intermediate.dense.weight', 'decoder.bert.encoder.layer.3.attention.self.key.weight', 'decoder.bert.encoder.layer.11.attention.self.query.bias', 'decoder.bert.encoder.layer.5.attention.self.query.bias', 'decoder.bert.encoder.layer.0.output.dense.bias', 'decoder.bert.encoder.layer.1.output.dense.bias', 'decoder.cls.predictions.decoder.weight', 'decoder.bert.encoder.layer.3.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.intermediate.dense.bias', 'decoder.bert.encoder.layer.7.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.attention.self.key.weight', 'decoder.bert.encoder.layer.11.attention.output.dense.weight', 'decoder.bert.encoder.layer.0.intermediate.dense.bias', 'decoder.bert.encoder.layer.4.attention.self.query.bias', 'decoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.intermediate.dense.weight', 'decoder.bert.encoder.layer.1.intermediate.dense.weight', 'decoder.bert.encoder.layer.9.output.dense.weight', 'decoder.bert.encoder.layer.2.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.output.dense.bias', 'decoder.bert.encoder.layer.5.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.attention.output.dense.bias', 'decoder.bert.encoder.layer.9.attention.self.value.weight', 'decoder.bert.embeddings.LayerNorm.weight', 'decoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'decoder.bert.embeddings.word_embeddings.weight', 'decoder.bert.encoder.layer.9.attention.self.query.bias', 'decoder.bert.encoder.layer.8.intermediate.dense.weight', 'decoder.bert.encoder.layer.3.attention.output.dense.bias', 'decoder.bert.encoder.layer.11.attention.self.query.weight', 'decoder.bert.embeddings.token_type_embeddings.weight', 'decoder.bert.encoder.layer.9.attention.self.query.weight', 'decoder.bert.encoder.layer.11.intermediate.dense.weight', 'decoder.bert.encoder.layer.6.attention.self.query.weight', 'decoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.output.dense.bias', 'decoder.bert.encoder.layer.10.output.dense.weight', 'decoder.bert.encoder.layer.7.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.attention.output.dense.weight', 'decoder.bert.encoder.layer.3.output.LayerNorm.weight', 'decoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.attention.self.key.weight', 'decoder.bert.encoder.layer.8.attention.self.key.weight', 'decoder.bert.encoder.layer.6.output.dense.bias', 'decoder.bert.encoder.layer.6.attention.self.value.bias', 'decoder.bert.encoder.layer.7.attention.self.key.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.10.output.LayerNorm.weight', 'decoder.bert.encoder.layer.4.attention.self.key.bias', 'decoder.bert.encoder.layer.3.output.dense.bias', 'decoder.bert.encoder.layer.0.output.LayerNorm.bias', 'decoder.bert.encoder.layer.4.output.LayerNorm.bias', 'decoder.bert.encoder.layer.11.output.dense.weight', 'decoder.bert.encoder.layer.8.attention.self.value.weight', 'decoder.bert.encoder.layer.3.attention.self.key.bias', 'decoder.bert.encoder.layer.1.attention.self.key.bias', 'decoder.bert.encoder.layer.8.intermediate.dense.bias', 'decoder.bert.encoder.layer.5.output.LayerNorm.weight', 'decoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.3.attention.self.value.weight', 'decoder.bert.encoder.layer.9.attention.self.value.bias', 'decoder.bert.encoder.layer.6.attention.self.value.weight', 'decoder.bert.encoder.layer.4.attention.output.dense.weight', 'decoder.bert.encoder.layer.8.output.LayerNorm.weight', 'decoder.bert.encoder.layer.11.attention.self.value.bias', 'decoder.bert.encoder.layer.2.intermediate.dense.bias', 'decoder.bert.encoder.layer.9.output.LayerNorm.weight', 'decoder.bert.encoder.layer.0.attention.self.key.weight', 'decoder.bert.encoder.layer.10.attention.output.dense.bias', 'decoder.bert.encoder.layer.4.output.dense.weight', 'decoder.bert.encoder.layer.8.attention.self.key.bias', 'decoder.bert.encoder.layer.8.output.dense.bias', 'decoder.bert.encoder.layer.4.attention.self.value.bias', 'decoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.5.intermediate.dense.weight', 'decoder.bert.encoder.layer.11.attention.self.value.weight', 'decoder.bert.encoder.layer.8.attention.self.value.bias', 'decoder.bert.encoder.layer.5.attention.output.dense.bias', 'decoder.bert.encoder.layer.2.attention.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.query.weight', 'decoder.bert.encoder.layer.9.attention.output.dense.weight', 'decoder.bert.encoder.layer.6.attention.self.key.weight', 'decoder.bert.encoder.layer.3.output.dense.weight', 'decoder.bert.encoder.layer.2.attention.self.query.weight', 'decoder.bert.encoder.layer.5.attention.self.value.bias', 'decoder.bert.encoder.layer.11.intermediate.dense.bias', 'decoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.8.output.LayerNorm.bias', 'decoder.bert.encoder.layer.10.attention.self.value.bias', 'decoder.bert.encoder.layer.3.intermediate.dense.weight', 'decoder.bert.encoder.layer.5.attention.self.key.bias', 'decoder.bert.encoder.layer.7.attention.self.query.bias', 'decoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.2.output.dense.bias', 'decoder.bert.encoder.layer.7.attention.self.value.weight', 'decoder.bert.encoder.layer.9.intermediate.dense.weight', 'decoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'decoder.bert.encoder.layer.6.attention.self.key.bias', 'decoder.bert.encoder.layer.2.attention.output.dense.weight', 'decoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'decoder.bert.encoder.layer.9.output.LayerNorm.bias', 'decoder.bert.encoder.layer.1.output.LayerNorm.bias', 'decoder.bert.encoder.layer.3.output.LayerNorm.bias', 'decoder.bert.encoder.layer.0.attention.self.value.weight', 'decoder.bert.encoder.layer.1.attention.self.query.weight', 'decoder.bert.encoder.layer.8.attention.output.dense.bias', 'decoder.bert.encoder.layer.3.attention.self.query.bias', 'decoder.bert.encoder.layer.1.attention.self.query.bias', 'decoder.bert.encoder.layer.0.attention.self.query.bias', 'decoder.bert.encoder.layer.1.attention.self.value.weight', 'decoder.bert.encoder.layer.7.attention.self.value.bias', 'decoder.bert.encoder.layer.7.intermediate.dense.bias', 'decoder.bert.encoder.layer.0.attention.self.key.bias', 'decoder.bert.encoder.layer.9.attention.self.key.weight', 'decoder.bert.encoder.layer.1.attention.output.dense.weight', 'decoder.bert.encoder.layer.2.output.dense.weight', 'decoder.bert.encoder.layer.1.attention.self.key.weight', 'decoder.bert.encoder.layer.5.attention.output.dense.weight', 'decoder.bert.encoder.layer.11.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, EncoderDecoderModel\n",
    "ckpt = 'mrm8488/bert2bert_shared-spanish-finetuned-summarization'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(ckpt)\n",
    "model = EncoderDecoderModel.from_pretrained(ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67efcd61",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    4,  1162,  2831,  1994,  1008,  5571, 10434,  1036,  1049,  6160,\n",
      "          1036, 23984,  2872,  1051,  1858,  1008,  1049,  2444,  1008,  1858,\n",
      "             5]], device='cuda:0')\n",
      "El expresidente de Chile participa en un encuentro en Idaho con personas de un grupo de personas\n"
     ]
    }
   ],
   "source": [
    "def generate_summary2(text):\n",
    "   inputs = tokenizer([text], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "   input_ids = inputs.input_ids.to(device)\n",
    "   attention_mask = inputs.attention_mask.to(device)\n",
    "   output = model.generate(input_ids, attention_mask=attention_mask)\n",
    "   print(output)\n",
    "   print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "   \n",
    "text = \"\"\"El \"campamento\" de Sun Valley en Idaho lo organiza anualmente la firma de inversión privada Allen&Company. En distintas versiones del evento se han cerrado importantes negocios. Sin embargo, \"lo que sucede en el interior del resort sigue siendo un misterio\", indicó un medio internacional. Economía Para niños, adolescentes, tercera edad, solteros y un sinfín de grupos. También para los millonarios. Los campamentos de verano son un panorama “clásico” para los ciudadanos de Estados Unidos y hasta ahí llegó el expresidente de Chile, Sebastián Piñera, para participar de uno -Sun Valley- que reunió a personas de ese último grupo. El campamento tuvo este 2023 su versión número 38; y su objetivo fue compartir experiencias en el ámbito de los negocios y tertulias sobre la economía, en un resort que tiene habitaciones de lujo, spa, salas de cine y otras comodidades, según detalló diario El País. “Inspiradora experiencia en Sun Valley” El expresidente Piñera fue uno de los participantes de esta última versión de Sun Valley, que comenzó el 12 de julio. De acuerdo a medios extranjeros, también asistieron Bill Gates de Microsoft; el fundador de Meta, Mark Zuckerberg; el director ejecutivo de Walt Disney Company, Bob Iger; entre otros ejecutivos. De hecho, el exmandatario compartió fotos en la instancia con los dos primeros; y detalló que también dialogó con “Jeff Bezos de Amazon; Sundar Pichai de Google, Marcos Galperin de Mercado Libre y Sam Altman de Chat GPT y muchos más”. Las imágenes muestran que viajó a la actividad junto a su esposa, Cecilia Morel.  En su publicación en la red social, Piñera calificó la experiencia como “inspiradora” y señaló -a propósito de su foto con Zuckerberg- que ya tiene una cuenta en Threads. El “campamento” de Sun Valley en Idaho lo organiza anualmente la firma de inversión privada Allen&Company. Según El País, en el evento se han cerrado importantes negocios, como -por ejemplo- “la compra de NBC Universal por parte de Comecast”. “Lo que sucede en el interior del resort, sin embargo, sigue siendo un misterio. Además de escuchar conferencias y participar en mesas redondas sobre política, tecnología o economía, los asistentes también son invitados a relajarse con diversas actividades recreativas, que van desde las largas caminatas por las montañas hasta golf”, indicó también el citado medio.\"\"\"\n",
    "generate_summary2(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9063d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
